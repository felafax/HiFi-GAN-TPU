{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5356ffde-84ad-41e2-99c2-dea58d69fc4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Package installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0325f8-6ad6-4add-b120-2bf5beb0f04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get update -y\n",
    "!apt-get install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a576756-f1b2-4ab1-bb15-9e5c5be23292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2b979-8f78-4076-ad7d-fddb493773c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/neel04/NeMo.git@dev#egg=nemo_toolkit[tts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fe598-40ff-4897-9ec0-93464bc30dab",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba1e03-a86f-44ce-bc4d-463e11e0d855",
   "metadata": {},
   "source": [
    "These are some debug flags. You can turn them on for performance benchmarking:\n",
    "\n",
    "```py\n",
    "%env TORCH_WARN=1\n",
    "%env PT_XLA_DEBUG_LEVEL=1\n",
    "%env PJRT_DEVICE=TPU\n",
    "%env TORCHDYNAMO_VERBOSE=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c019d85-e2f9-4a44-a936-d7f512dd8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import soundfile as sf\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.models import HifiGanModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c909187-e018-4730-b1e5-98c8cc8b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logging.getLogger(\"nemo_logger\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eaa2f9-7144-4780-85fd-29fa5bf1365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torchvision\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from IPython.display import Audio\n",
    "from dataclasses import dataclass\n",
    "from contextlib import ContextDecorator\n",
    "\n",
    "device = torch_xla.device()\n",
    "\n",
    "torch_xla.experimental.eager_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0da39-f30e-4c6b-8714-a9d52f8e37ed",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834eea0-0c60-430e-85a3-87129f315670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf ./compilation_cache/\n",
    "import torch_xla.runtime as xr\n",
    "xr.initialize_cache('./compilation_cache', readonly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631b950-95b6-4004-bb5f-ebc733351908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec_generator = FastPitchModel.from_pretrained(\"nvidia/tts_en_fastpitch\").to(device)\n",
    "model = HifiGanModel.from_pretrained(model_name=\"nvidia/tts_hifigan\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fac74c-64b7-4065-95f9-e39890408d41",
   "metadata": {},
   "source": [
    "Next, we convert the model weights to fp16/bf16. This utility function can also be configured to ignore certain weights which are better in full precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b204b5f-a4d4-4809-8f57-9d24fcd5a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dtype(model, dtype=torch.float16):\n",
    "    # Convert model parameters to FP16\n",
    "    model = model.to(dtype)\n",
    "    \n",
    "    # Keep norm layers in FP32 (for future models)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bn' in name:\n",
    "            param.data = param.data.float()\n",
    "            param.grad = param.grad.float()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = convert_to_dtype(model)\n",
    "spec_generator = convert_to_dtype(spec_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d79573-07a4-4ffb-a87d-2fbaf8506e27",
   "metadata": {},
   "source": [
    "Setting the model to `eval` (inferencing) mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2dfcce-6fcf-4126-9f8c-acec7cf90786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec_generator = spec_generator.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "spec_generator.eval()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bae2b-f94e-47fd-a084-d54557d45449",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f067233-e944-4425-9caa-e8d60db0d200",
   "metadata": {},
   "source": [
    "Here, we expose some functions for debugging & optimization reasons. One may need to partially compile some parts of the model depending on the architecture, usecase and performance. Thus we expose some utilities that we modify for better performance on TPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c8c4d-f975-4aea-bec2-6412d7727b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(backend=\"openxla\", fullgraph=True)\n",
    "def log_to_duration(log_dur, min_dur, max_dur, mask):\n",
    "    dur = torch.clamp(torch.exp(log_dur) - 1.0, min_dur, max_dur)\n",
    "    dur *= mask.squeeze(2)\n",
    "    return dur\n",
    "    \n",
    "@torch.compile(backend='openxla', fullgraph=True)\n",
    "def compiled_device_transfer(x, device):\n",
    "    return x.to(device)\n",
    "    \n",
    "def tensor_to_int(x: torch.Tensor) -> int:\n",
    "    '''\n",
    "    Casts tensors to integer and places them on CPU \n",
    "    '''\n",
    "    return x.cpu().detach().long().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783dc558-3551-4bbc-b231-68b3ccf63cc9",
   "metadata": {},
   "source": [
    "This is simply a context manager for easy benchmarking of torch XLA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4fe31f-706e-4da6-989a-50ab0fbee39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    time_ms: float\n",
    "    time_sec: float\n",
    "    output: Optional[any] = None\n",
    "\n",
    "class benchmark_xla(ContextDecorator):\n",
    "    def __init__(self, name: str = \"Benchmark\", silent: bool = False):\n",
    "        self.name = name\n",
    "        self.silent = silent\n",
    "        self.result: Optional[BenchmarkResult] = None\n",
    "        self.output = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *exc):\n",
    "        xm.mark_step()\n",
    "        xm.wait_device_ops()\n",
    "            \n",
    "        time_taken = time.time() - self.start_time\n",
    "        \n",
    "        self.result = BenchmarkResult(\n",
    "            time_ms=round(time_taken * 1000, 3),\n",
    "            time_sec=round(time_taken, 3),\n",
    "            output=self.output\n",
    "        )\n",
    "        \n",
    "        if not self.silent:\n",
    "            print(f\"{self.name} - Time taken: {self.result.time_ms} ms ({self.result.time_sec} seconds)\")\n",
    "            \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174322d-d5d2-414c-a6d6-90bf904ab091",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69629416-ca2c-4ee7-b855-36a513bc82ec",
   "metadata": {},
   "source": [
    "This function isn't `JIT`-compiled, however it does consume a static max length. Make sure inputs aren't clipped due to the static length. A max-length of `768-1024` should be good enough for <10s of audio, suiting most use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42072ac2-ce9b-4d5c-a1e5-670ccd6adb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fastpitch = spec_generator.fastpitch # convenience\n",
    "STATIC_MAX_LENGTH: int = 768\n",
    "PARSED_PADLEN: int = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cc3fd-ea68-4ea9-a684-d83945ef2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_regulate_len(\n",
    "    durations,\n",
    "    enc_out,\n",
    "    pace: float = 1.0,\n",
    "    group_size: int = 1,\n",
    "    dur_lens: torch.tensor = None,\n",
    "    max_allowed_len: int = STATIC_MAX_LENGTH,\n",
    "):\n",
    "    \"\"\"XLA-optimized version of regulate_len that minimizes dynamic operations.\n",
    "    Uses a pre-defined maximum length to avoid dynamic arange operations.\"\"\"\n",
    "    dtype = enc_out.dtype\n",
    "    device = enc_out.device\n",
    "    \n",
    "    # Static division and floor operations\n",
    "    reps = (durations.float() / pace + 0.5).floor()\n",
    "    dec_lens = reps.sum(dim=1)\n",
    "    \n",
    "    # Instead of computing max_len dynamically, use max_allowed_len\n",
    "    # This ensures static shape for XLA\n",
    "    \n",
    "    # Pre-compute pad and cumsum in a more static way\n",
    "    padded_reps = torch.nn.functional.pad(reps, (1, 0, 0, 0), value=0.0)\n",
    "    reps_cumsum = torch.cumsum(padded_reps, dim=1)[:, None, :]\n",
    "    reps_cumsum = reps_cumsum.to(dtype=dtype, device=device)\n",
    "    \n",
    "    # Use static pre-computed range tensor instead of dynamic arange\n",
    "    # This could be moved outside the function if needed\n",
    "    static_range = torch.arange(max_allowed_len, device=device)[None, :, None]\n",
    "    \n",
    "    # Compute multiplication mask with fewer dynamic operations\n",
    "    mult = (reps_cumsum[:, :, :-1] <= static_range) & (reps_cumsum[:, :, 1:] > static_range)\n",
    "    mult = mult.to(dtype)\n",
    "    \n",
    "    # Final matrix multiplication\n",
    "    enc_rep = torch.matmul(mult, enc_out)\n",
    "    \n",
    "    # Trim to actual length needed\n",
    "    length_mask = torch.arange(enc_rep.shape[1], device=enc_rep.device)[None, :] < dec_lens[:, None]\n",
    "    enc_rep = enc_rep * length_mask.unsqueeze(-1)\n",
    "\n",
    "    return enc_rep, dec_lens, dec_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c98aa-0284-454d-bcef-76b7cb877728",
   "metadata": {},
   "source": [
    "`fp_prediction` is the fused inference function which consumes a `parsed: Tensor` textual input and returns the raw (padded) audio waveform. \n",
    "\n",
    "We also propogate `real_len: int` output which can be used to reconstruct how much we need to slice off the raw audio waveform to remove padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc309eb-b5ed-433a-b49f-1eeb1eaa02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(backend='openxla', fullgraph=True)\n",
    "def fp_prediction(parsed: torch.Tensor, conditioning=None):\n",
    "    with torch.no_grad():\n",
    "        parsed = parsed.to(device)     \n",
    "        enc_out, enc_mask = Fastpitch.encoder(input=parsed, conditioning=conditioning)\n",
    "        enc_out, enc_mask = enc_out.to(torch.float16), enc_mask.to(torch.float16)\n",
    "        \n",
    "        # Duration prediction\n",
    "        log_durs_predicted = Fastpitch.duration_predictor(enc_out, enc_mask, conditioning=conditioning)\n",
    "        durs_predicted = log_to_duration(\n",
    "            log_dur=log_durs_predicted, \n",
    "            min_dur=Fastpitch.min_token_duration, \n",
    "            max_dur=Fastpitch.max_token_duration, \n",
    "            mask=enc_mask\n",
    "        )\n",
    "        \n",
    "        # Pitch prediction\n",
    "        pitch_predicted = Fastpitch.pitch_predictor(enc_out, enc_mask, conditioning=conditioning)\n",
    "        pitch_emb = Fastpitch.pitch_emb(pitch_predicted.unsqueeze(1))\n",
    "        \n",
    "        # Combine encoder output and pitch embedding\n",
    "        enc_out = enc_out + pitch_emb.transpose(1, 2)\n",
    "        \n",
    "        len_regulated, dec_lens, real_len = static_regulate_len(durs_predicted, enc_out, pace=1.0)\n",
    "        dec_out, _ = Fastpitch.decoder(input=len_regulated, seq_lens=dec_lens, conditioning=conditioning)\n",
    "        dec_out = dec_out.to(torch.float16)\n",
    "        \n",
    "        spect = Fastpitch.proj(dec_out).transpose(1, 2) # Obtain spectrogram\n",
    "\n",
    "        audio = model.convert_spectrogram_to_audio(spec=spect)\n",
    "\n",
    "        audio, durs_predicted = audio.to('cpu'), durs_predicted.to('cpu')\n",
    "\n",
    "        return audio, real_len, durs_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b495056-e48c-46b3-99b0-4092ed11ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text: str | list[str], padlen: int = PARSED_PADLEN) -> tuple[torch.Tensor, list[int]]:\n",
    "    torch.set_default_device('cpu')\n",
    "\n",
    "    pre_pad_lens = torch.zeros(len(text))\n",
    "    text = [text] if isinstance(text, str) else text # wrap it in a list if required\n",
    "    out = torch.zeros(len(text), padlen) # preallocate the output\n",
    "\n",
    "    for index, t in enumerate(text):\n",
    "        parsed = spec_generator.parse(t).cpu()\n",
    "        pre_pad_len = parsed.shape[1]\n",
    "        \n",
    "        if pre_pad_len > padlen:\n",
    "            print(f'WARNING: Input padding is insufficient for text size. Recommend doublind paddling length.')\n",
    "            \n",
    "        out[index, :] = F.pad(parsed.cpu(), (0, padlen - parsed.shape[1]), value=0).long()\n",
    "        pre_pad_lens[index] = pre_pad_len\n",
    "\n",
    "    return out.long().cpu(), pre_pad_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45184a84-b57a-44b8-8e06-d379c3bc2311",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f781847-2c8f-46dd-bc64-bf08c5e569f5",
   "metadata": {},
   "source": [
    "`infer_e2e` is the exposed function that directly consumes `text: str` (CPU) performs the model forward pass (TPU) and produces the output raw (padded) `audio: Tensor` waveform (CPU). Thus, the benchmarking numbers take data-movement in account as its often the bottleneck for latency-critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e1be5-7ba0-4845-a5a6-714b4ca0710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_e2e(text: list[str], parsed: torch.Tensor | None = None) -> tuple[torch.Tensor, int]:\n",
    "    '''\n",
    "    This function can optionally consume a directly parsed tensor\n",
    "    for debugging purposes.\n",
    "    '''\n",
    "    pre_pad_len: int | None = None\n",
    "    \n",
    "    if parsed is None:\n",
    "        assert type(text) is list, f'Invalid input type. Got: {type(text)} expected a list'\n",
    "        parsed, pre_pad_len = parse_text(text)\n",
    "\n",
    "    audio, real_len, durs_predicted = fp_prediction(parsed.to(device))\n",
    "    \n",
    "    return audio, pre_pad_len, real_len, durs_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f4b39-eefa-4d0f-b72d-2101b85e3670",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30238bdb-9d13-442f-82bb-0561d08912a0",
   "metadata": {},
   "source": [
    "We do a **compilation warmup** here to reduce the chances of compilation cache misses. Re-compilations take $> 800-1200 \\text{ms}$ thus we want to avoid them as much as possible. \n",
    "Note that there still may be some recompilations (which would be obvious from the latency) but with a good enough warmup plus continued usage, those issues should be alleviated as a good enough cache is constructed.\n",
    "\n",
    "Cache resides in the directly `./compilation_cache` and should be cleared periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3d776-0d48-4630-ab85-0d47bf2fe017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_random_string(length):\n",
    "    chars = string.ascii_letters + string.digits + string.punctuation\n",
    "    return ''.join(random.choice(chars) for _ in range(length))\n",
    "\n",
    "def warmup_parse_text():\n",
    "    for length in tqdm(range(128)):\n",
    "        text = generate_random_string(length)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                out = parse_text(text)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "warmup_parse_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af99a2-df0b-43aa-a68e-e5a51f5fe946",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = [\n",
    "    \"The bees decided to have a mutiny against their queen, But they forgot to collect the honey.\",\n",
    "    \"When he had to picnic on the beach, he purposely put sand in other people’s food.\",\n",
    "    \"The gruff old man sat in the back of the bait shop grumbling to himself as he scooped out a handful of worms.\",\n",
    "    \"The Tsunami wave crashed against the raised houses and broke the pilings as if they were toothpicks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaada462-bfeb-4252-9a18-d021b2760937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_parsed = torch.randint(0, 500, (1, 128)).long().to(device)\n",
    "raw_audio, pre_pad_len, real_len, durs_predicted = infer_e2e(text = None, parsed = _parsed)\n",
    "pred_noise_duration = (real_len - durs_predicted[:, pre_pad_len:].sum()) * 256\n",
    "\n",
    "with benchmark_xla('Warmup benchmark') as e2e_bench:\n",
    "    text = \"This is the ritual to lead you on; your friends would meet you when your gone.\"\n",
    "    raw_audio, pre_pad_len, real_len, durs_predicted = infer_e2e(text = [text])\n",
    "    pred_noise_duration = (real_len - durs_predicted[:, pre_pad_len.long():].sum()) * 256\n",
    "\n",
    "\n",
    "with benchmark_xla('Warmup Benchmark #2') as e2e_bench:\n",
    "    raw_audio, pre_pad_len, real_len, durs_predicted = infer_e2e(text_batch)\n",
    "    \n",
    "    pred_noise_duration = torch.tensor([\n",
    "        (real_len[i] - durs_predicted[i, pre_pad_len[i].long():].sum()) * 256 \n",
    "        for i in range(4)\n",
    "    ], device=durs_predicted.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a40d9e-cb74-4c92-9c39-3948ff240660",
   "metadata": {},
   "source": [
    "## End-to-end inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f8fe5-f6d5-429a-8c56-b27544bb155d",
   "metadata": {},
   "source": [
    "This is the `text` we wish to convert to audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3db1cc-9ae0-48ba-8749-637506b1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The employee wanted to request his employers for a raise, but was unable to do so because he feared his immediate expulsion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714f1e5-4830-4d94-8dea-b10a7ac56b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark_xla('End-to-end forward pass') as e2e_bench:\n",
    "    raw_audio, pre_pad_len, real_len, durs_predicted = infer_e2e([text])\n",
    "    pred_noise_duration = (real_len - durs_predicted[:, pre_pad_len.long():].sum()) * 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf8cb0-50d2-456d-8a99-057b28d3cff1",
   "metadata": {},
   "source": [
    "One aspect we wish to draw attention upon is the parsing operation of the text itself (performed by `parse_text`) is often the bottleneck, taking up majority of the time-taken ($50-60\\%$). The actual forward pass timing is thus multiple factors less than the timing provided above. With further optimizations (lowering the parsing operation into more performance system languages like Rust or C++) we can reduce the latency even more if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ece84b-b2fa-47ca-b606-e6ee901f697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark_xla('Text Parsing') as text_parsing:\n",
    "    out = parse_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624b078-97a4-4c3c-9a09-f8dd33f09002",
   "metadata": {},
   "source": [
    "Here, we provide a utility to convert this raw audio waveform to an Ipython-embedded widget for easy playback on the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0bd22-49ab-4373-8bf3-8325318a8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = raw_audio.cpu().detach().numpy()[:, :tensor_to_int(pred_noise_duration)]\n",
    "Audio(audio, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8d00a-e017-4376-af75-bdeff5344397",
   "metadata": {},
   "source": [
    "## Batched Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b95a5b-9b50-4c22-afdf-6172748778f7",
   "metadata": {},
   "source": [
    "We also support batched inference with arbitrary batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8daab-d76f-48c7-a48e-eabfcf8099f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"No matter how beautiful the sunset, it saddened her knowing she was one day older.\",\n",
    "    \"As time wore on, simple dog commands turned into full paragraphs explaining why the dog couldn’t do something.\",\n",
    "    \"She found it strange that people use their cellphones to actually talk to one another.\",\n",
    "    \"His ultimate dream fantasy consisted of being content and sleeping eight hours in a row.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18352271-e3fa-40a0-a8e8-e07ae8ee412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark_xla('End-to-end forward pass') as e2e_bench:\n",
    "    raw_audio, pre_pad_len, real_len, durs_predicted = infer_e2e(text)\n",
    "    \n",
    "    pred_noise_duration = torch.tensor([\n",
    "        (real_len[i] - durs_predicted[i, pre_pad_len[i].long():].sum()) * 256 \n",
    "        for i in range(4)\n",
    "    ], device=durs_predicted.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025cce3-87ba-4e6b-b941-903928b93a78",
   "metadata": {},
   "source": [
    "Again, we measure how much time text parsing takes in the batched case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4d046-ecb6-47cd-8d17-d6203fde930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with benchmark_xla('Text Parsing') as text_parsing:\n",
    "    out = parse_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d7d32-b697-418e-a727-88fa3058ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "index: int = 0 # select the index of the audio that would be played \n",
    "\n",
    "audio = raw_audio.cpu().detach().numpy()[index, :tensor_to_int(pred_noise_duration[index])]\n",
    "Audio(audio, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f0d4b-88db-4d7b-873d-e15e34cb224f",
   "metadata": {},
   "source": [
    "# Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47496953-c2f6-4eba-b3d1-6810651bf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_LENGTH_SECONDS = 8 * len(text)\n",
    "\n",
    "time_taken = e2e_bench.result.time_ms\n",
    "e2e_parsing_time_taken = text_parsing.result.time_ms\n",
    "rtfx = (AUDIO_LENGTH_SECONDS / time_taken) * 1000\n",
    "rtfx_without_parsing = (AUDIO_LENGTH_SECONDS / (time_taken - e2e_parsing_time_taken)) * 1000\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"  📊 Performance Metrics | Batch size: {len(text)}\")\n",
    "print(\"=\"*50)\n",
    "print(f\" ⏱️  Total Time       : {time_taken:>8.2f} ms\")\n",
    "print(f\" 🚀  RTFx             : {rtfx:>8.2f}x\")\n",
    "print(f\" 🔥  RTFx (no parse)  : {rtfx_without_parsing:>8.2f}x\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
